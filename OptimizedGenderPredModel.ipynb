{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1967f06-2c76-4906-a461-b4a45d17a6b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4419d5cd10e44105b810f4090afa1240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23710 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbbeff6a4c4c4b0a81f5cdc5a8db32f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23709 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature shape: (23709, 48, 48, 3)\n",
      "Training set: (18967, 48, 48, 3), Test set: (4742, 48, 48, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">48</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">DepthwiseConv2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">65,792</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gender_out (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">129</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d (\u001b[38;5;33mDepthwiseConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m3\u001b[0m)           │              \u001b[38;5;34m30\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m48\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d_1 (\u001b[38;5;33mDepthwiseConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │             \u001b[38;5;34m640\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m8,320\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ depthwise_conv2d_2 (\u001b[38;5;33mDepthwiseConv2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)         │           \u001b[38;5;34m1,280\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │          \u001b[38;5;34m33,024\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │           \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling2d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │          \u001b[38;5;34m65,792\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ gender_out (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │             \u001b[38;5;34m129\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">144,159</span> (563.12 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m144,159\u001b[0m (563.12 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">143,263</span> (559.62 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m143,263\u001b[0m (559.62 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> (3.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m896\u001b[0m (3.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Parameters: 144,159 (1.4 lakh)\n",
      "Epoch 1/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 144ms/step - accuracy: 0.7103 - loss: 0.5588 - val_accuracy: 0.5228 - val_loss: 0.6907 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 142ms/step - accuracy: 0.8118 - loss: 0.4171 - val_accuracy: 0.8248 - val_loss: 0.4464 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m524s\u001b[0m 2s/step - accuracy: 0.8417 - loss: 0.3585 - val_accuracy: 0.7676 - val_loss: 0.4624 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 156ms/step - accuracy: 0.8560 - loss: 0.3359 - val_accuracy: 0.7950 - val_loss: 0.4795 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 158ms/step - accuracy: 0.8675 - loss: 0.3099 - val_accuracy: 0.8484 - val_loss: 0.3432 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 142ms/step - accuracy: 0.8727 - loss: 0.2937 - val_accuracy: 0.8568 - val_loss: 0.3238 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 145ms/step - accuracy: 0.8797 - loss: 0.2742 - val_accuracy: 0.8648 - val_loss: 0.3014 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 145ms/step - accuracy: 0.8910 - loss: 0.2518 - val_accuracy: 0.8269 - val_loss: 0.3559 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 142ms/step - accuracy: 0.8929 - loss: 0.2459 - val_accuracy: 0.8652 - val_loss: 0.3088 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 143ms/step - accuracy: 0.9018 - loss: 0.2293 - val_accuracy: 0.8728 - val_loss: 0.2895 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 142ms/step - accuracy: 0.9073 - loss: 0.2137 - val_accuracy: 0.8534 - val_loss: 0.3195 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 143ms/step - accuracy: 0.9093 - loss: 0.2137 - val_accuracy: 0.8798 - val_loss: 0.2970 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 145ms/step - accuracy: 0.9215 - loss: 0.1867 - val_accuracy: 0.8787 - val_loss: 0.2933 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 398ms/step - accuracy: 0.9264 - loss: 0.1767 - val_accuracy: 0.8054 - val_loss: 0.6341 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 170ms/step - accuracy: 0.9286 - loss: 0.1702 - val_accuracy: 0.8699 - val_loss: 0.3599 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 158ms/step - accuracy: 0.9323 - loss: 0.1586 - val_accuracy: 0.8452 - val_loss: 0.3861 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 149ms/step - accuracy: 0.9384 - loss: 0.1494 - val_accuracy: 0.8712 - val_loss: 0.3248 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m526s\u001b[0m 2s/step - accuracy: 0.9556 - loss: 0.1040 - val_accuracy: 0.8834 - val_loss: 0.3817 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m297/297\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 227ms/step - accuracy: 0.9607 - loss: 0.0944 - val_accuracy: 0.8785 - val_loss: 0.3643 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m 30/297\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 219ms/step - accuracy: 0.9769 - loss: 0.0608 "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Dropout, MaxPooling2D, Input, BatchNormalization, DepthwiseConv2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "\n",
    "# ===============================\n",
    "# Load Dataset\n",
    "# ===============================\n",
    "BASE_DIR = r\"C:\\Users\\Dishang\\OneDrive\\Desktop\\UTKFace\"\n",
    "\n",
    "image_paths = []\n",
    "age_labels = []\n",
    "gender_labels = []\n",
    "\n",
    "for filename in tqdm(os.listdir(BASE_DIR)):\n",
    "    try:\n",
    "        image_path = os.path.join(BASE_DIR, filename)\n",
    "        temp = filename.split('_')\n",
    "        age = int(temp[0])\n",
    "        gender = int(temp[1])\n",
    "        image_paths.append(image_path)\n",
    "        age_labels.append(age)\n",
    "        gender_labels.append(gender)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['image'], df['age'], df['gender'] = image_paths, age_labels, gender_labels\n",
    "\n",
    "gender_dict = {0: 'Male', 1: 'Female'}\n",
    "\n",
    "# ===============================\n",
    "# Feature Extraction\n",
    "# ===============================\n",
    "def extract_features_optimized(images, target_size=(48, 48)):\n",
    "    features = []\n",
    "    for image in tqdm(images):\n",
    "        try:\n",
    "            img = load_img(image, color_mode='rgb')\n",
    "            img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "            img = np.array(img)\n",
    "            features.append(img)\n",
    "        except:\n",
    "            continue\n",
    "    features = np.array(features)\n",
    "    return features\n",
    "\n",
    "X = extract_features_optimized(df['image'], target_size=(48, 48))\n",
    "print(f\"Feature shape: {X.shape}\")\n",
    "\n",
    "# Normalize(#17_10)\n",
    "X = X.astype(np.float32) / 255.0\n",
    "X = X * 2.0 - 1.0   # scale to [-1, 1] for INT8 quantization\n",
    "\n",
    "\n",
    "# Labels\n",
    "y_gender = np.array(df['gender'][:len(X)])\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_gender_train, y_gender_test = train_test_split(\n",
    "    X, y_gender, test_size=0.2, random_state=42, stratify=y_gender\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# Model Architecture\n",
    "# ===============================\n",
    "def create_high_accuracy_model():\n",
    "    inputs = Input((48,48,3))\n",
    "\n",
    "    # Conv Blocks\n",
    "    x = DepthwiseConv2D((3,3), padding='same', activation='relu')(inputs)\n",
    "    x = Conv2D(64, (1,1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = DepthwiseConv2D((3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(128, (1,1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = DepthwiseConv2D((3,3), padding='same', activation='relu')(x)\n",
    "    x = Conv2D(256, (1,1), activation='relu', padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D()(x)\n",
    "\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Dense Layers\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "\n",
    "    outputs = Dense(1, activation='sigmoid', name='gender_out')(x)\n",
    "    return Model(inputs, outputs)\n",
    "\n",
    "\n",
    "model = create_high_accuracy_model()\n",
    "\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "total_params = model.count_params()\n",
    "print(f\"\\nTotal Parameters: {total_params:,} ({total_params/100000:.1f} lakh)\")\n",
    "\n",
    "# ===============================\n",
    "# Callbacks\n",
    "# ===============================\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        mode='min'\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        mode='max'\n",
    "    )\n",
    "]\n",
    "\n",
    "# ===============================\n",
    "# Training\n",
    "# ===============================\n",
    "history = model.fit(\n",
    "    X_train, y_gender_train,\n",
    "    batch_size=64,\n",
    "    epochs=50,\n",
    "    validation_data=(X_test, y_gender_test),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"Training completed! Total epochs run: {len(history.history['loss'])}\")\n",
    "\n",
    "# ===============================\n",
    "# Evaluation\n",
    "# ===============================\n",
    "loss, acc = model.evaluate(X_test, y_gender_test, verbose=0)\n",
    "print(f\"\\nTest Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "# ===============================\n",
    "# Plot History\n",
    "# ===============================\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Training & Validation Accuracy\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# Sample Predictions\n",
    "# ===============================\n",
    "def test_predictions(model, X_test, y_gender_test, num_samples=5):\n",
    "    indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "    plt.figure(figsize=(15, 3*num_samples))\n",
    "\n",
    "    for i, idx in enumerate(indices):\n",
    "        pred = model.predict(X_test[idx:idx+1], verbose=0)\n",
    "        pred_gender = round(pred[0][0])\n",
    "        conf = pred[0][0]\n",
    "\n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.imshow(X_test[idx])\n",
    "        plt.title(f\"True: {gender_dict[y_gender_test[idx]]} | Pred: {gender_dict[pred_gender]} | Conf: {conf:.3f}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nSample Predictions:\")\n",
    "test_predictions(model, X_test, y_gender_test)\n",
    "\n",
    "# ===============================\n",
    "# Final Summary\n",
    "# ===============================\n",
    "final_train_acc = max(history.history['accuracy'])\n",
    "final_val_acc = max(history.history['val_accuracy'])\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Total Parameters: {total_params:,} ({total_params/100000:.1f} lakh)\")\n",
    "print(f\"Best Training Accuracy: {final_train_acc:.4f} ({final_train_acc*100:.2f}%)\")\n",
    "print(f\"Best Validation Accuracy: {final_val_acc:.4f} ({final_val_acc*100:.2f}%)\")\n",
    "print(f\"Test Accuracy: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "print(f\"{'='*50}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dead2f0f-381c-4c7b-b74b-78da1405a3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Dishang\\AppData\\Local\\Temp\\tmp3s43aqg9\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Dishang\\AppData\\Local\\Temp\\tmp3s43aqg9\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Dishang\\AppData\\Local\\Temp\\tmp3s43aqg9'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name='keras_tensor_19')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2155835715856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835715280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835714128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835716816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835717968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835714704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835715664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835714896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835715472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835716048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835713168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835713744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835712208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835712016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835714512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835712976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835713360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835713936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835711248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835711824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835712592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835710480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835710672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835711632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835710864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835711056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835712400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835708944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835709520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2155835709904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ INT8 quantized model saved successfully!\n",
      "Model size: 175.88 KB\n"
     ]
    }
   ],
   "source": [
    "# Convert to fully quantized TFLite (INT8)\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "# Representative dataset for calibration (take few training samples)\n",
    "def representative_dataset():\n",
    "    for i in range(100):\n",
    "        img = X_train[i].astype(np.float32)\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        yield [img]\n",
    "\n",
    "converter.representative_dataset = representative_dataset\n",
    "\n",
    "# Force full int8 quantization\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "# Convert\n",
    "tflite_quant_model = converter.convert()\n",
    "\n",
    "# Save the smaller model\n",
    "with open(\"gender_model_int8.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_quant_model)\n",
    "\n",
    "print(\"✅ INT8 quantized model saved successfully!\")\n",
    "\n",
    "# Check size\n",
    "size = os.path.getsize(\"gender_model_int8.tflite\") / 1024\n",
    "print(f\"Model size: {size:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74d4917a-508a-48cb-a44c-974ccf08c2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ===============================\n",
    "# Load the INT8 TFLite model\n",
    "# ===============================\n",
    "interpreter = tf.lite.Interpreter(model_path=\"gender_model_int8.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Input shape (e.g., [1, 48, 48, 3])\n",
    "input_shape = input_details[0]['shape']\n",
    "img_height, img_width = input_shape[1], input_shape[2]\n",
    "\n",
    "# ===============================\n",
    "# Helper: Predict gender\n",
    "# ===============================\n",
    "def predict_gender_tflite(face_img):\n",
    "    # Resize and normalize\n",
    "    face_img = cv2.resize(face_img, (img_width, img_height))\n",
    "    face_img = face_img.astype(np.float32) / 255.0\n",
    "\n",
    "    # Quantize input to int8 if model expects int8\n",
    "    if input_details[0]['dtype'] == np.int8:\n",
    "        input_scale, input_zero_point = input_details[0]['quantization']\n",
    "        face_img = face_img / input_scale + input_zero_point\n",
    "        face_img = face_img.astype(np.int8)\n",
    "\n",
    "    face_img = np.expand_dims(face_img, axis=0)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], face_img)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "    # Dequantize output if int8\n",
    "    if output_details[0]['dtype'] == np.int8:\n",
    "        output_scale, output_zero_point = output_details[0]['quantization']\n",
    "        output_data = (output_data.astype(np.float32) - output_zero_point) * output_scale\n",
    "\n",
    "    pred = output_data[0]\n",
    "    gender_label = \"Female\" if pred >= 0.5 else \"Male\"\n",
    "    confidence = pred if pred >= 0.5 else 1 - pred\n",
    "    return gender_label, float(confidence)\n",
    "\n",
    "# ===============================\n",
    "# Load Haar Cascade for Face Detection\n",
    "# ===============================\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# ===============================\n",
    "# Real-Time Webcam Feed\n",
    "# ===============================\n",
    "cap = cv2.VideoCapture(0)  # Change index if using external camera\n",
    "\n",
    "print(\"Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        gender, conf = predict_gender_tflite(face)\n",
    "\n",
    "        label = f\"{gender} ({conf*100:.1f}%)\"\n",
    "        color = (0, 255, 0) if gender == \"Male\" else (255, 0, 255)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(frame, label, (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(\"Gender Classification (INT8 Model)\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8719ac93-10ce-40f6-9221-3368f1475ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Header file 'gender_model_int8.h' created successfully!\n",
      "📦 Model size: 175.77 KB\n"
     ]
    }
   ],
   "source": [
    "#17_10\n",
    "import os\n",
    "\n",
    "# Path to your quantized TFLite model\n",
    "tflite_model_path = \"gender_model_int8.tflite\"\n",
    "\n",
    "# Read the TFLite model as bytes\n",
    "with open(tflite_model_path, \"rb\") as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "# Output header file name\n",
    "header_file = \"gender_model_int8.h\"\n",
    "\n",
    "# Variable name for model data in C\n",
    "var_name = os.path.splitext(os.path.basename(header_file))[0]\n",
    "\n",
    "# Create the header file\n",
    "with open(header_file, \"w\") as f:\n",
    "    f.write(\"// =====================================================\\n\")\n",
    "    f.write(\"//  TensorFlow Lite INT8 Model for Arduino Nano 33 BLE\\n\")\n",
    "    f.write(\"//  Automatically generated header file\\n\")\n",
    "    f.write(\"// =====================================================\\n\\n\")\n",
    "\n",
    "    f.write(f\"#ifndef {var_name.upper()}_H_\\n\")\n",
    "    f.write(f\"#define {var_name.upper()}_H_\\n\\n\")\n",
    "\n",
    "    # Model length\n",
    "    f.write(f\"unsigned int {var_name}_len = {len(tflite_model)};\\n\")\n",
    "    f.write(f\"const unsigned char {var_name}[] = {{\\n\")\n",
    "\n",
    "    # Write model bytes in hex format\n",
    "    for i, byte in enumerate(tflite_model):\n",
    "        if i % 12 == 0:\n",
    "            f.write(\"\\n \")\n",
    "        f.write(f\"0x{byte:02x}, \")\n",
    "\n",
    "    f.write(\"\\n};\\n\\n\")\n",
    "    f.write(f\"#endif // {var_name.upper()}_H_\\n\")\n",
    "\n",
    "print(f\"✅ Header file '{header_file}' created successfully!\")\n",
    "print(f\"📦 Model size: {len(tflite_model)/1024:.2f} KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db6264b-d599-4887-ba84-9232000985e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# Resize function to match model input\n",
    "def preprocess_face(face_img, target_size=(48,48)):\n",
    "    face_img = cv2.resize(face_img, target_size, interpolation=cv2.INTER_LANCZOS4)\n",
    "    face_img = face_img / 255.0\n",
    "    face_img = np.expand_dims(face_img, axis=0)\n",
    "    return face_img\n",
    "\n",
    "# Function to detect faces and predict gender\n",
    "def detect_and_predict_gender(image, model):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img = image[y:y+h, x:x+w]\n",
    "        face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)  # convert to RGB\n",
    "        face_input = preprocess_face(face_rgb, target_size=(48,48))\n",
    "        \n",
    "        pred = model.predict(face_input, verbose=0)[0][0]\n",
    "        gender_label = \"Female\" if pred >= 0.5 else \"Male\"\n",
    "        confidence = pred if pred >= 0.5 else 1 - pred\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(image, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv2.putText(image, f\"{gender_label} ({confidence*100:.1f}%)\", \n",
    "                    (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# --- Example: Real-time Webcam Gender Detection ---\n",
    "cap = cv2.VideoCapture(0)  # 0 for default webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    output_frame = detect_and_predict_gender(frame, model)\n",
    "    cv2.imshow(\"Gender Detection\", output_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c612e8b-0476-48e1-95fe-c79c215105bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model in HDF5 format\n",
    "model.save(\"optimized_17_10_gender_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73dceab-9f33-4c86-a03d-506e737f1c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Dishang\\AppData\\Local\\Temp\\tmpf7trcng6\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Dishang\\AppData\\Local\\Temp\\tmpf7trcng6\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\Dishang\\AppData\\Local\\Temp\\tmpf7trcng6'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 48, 48, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2152287177040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287177616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287177808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287177424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287179344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287176656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287178384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287178960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287179920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287179728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287180496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287177232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287179536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287181264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287181072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287178192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287182032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287182608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287183760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287182992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287184144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287185104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287184912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287184336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287184720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287183952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287182416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287186064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287182800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2152287185680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "TFLite model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Convert to TFLite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]  # Enable optimization for size and speed\n",
    "tflite_model = converter.convert()\n",
    "# Save the TFLite model\n",
    "with open(\"gender_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "print(\"TFLite model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8a25f19-f979-4cbb-8c9c-02cf0f088608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hex_to_c_array(hex_data, var_name):\n",
    "    c_str=''\n",
    "\n",
    "\n",
    "    c_str += '#ifndef' + var_name.upper() + '_H\\n'\n",
    "    c_str += '#define' + var_name.upper() + '_H\\n\\n'\n",
    "\n",
    "    c_str+='\\nunsigned int' +var_name + '_len =' +str(len(hex_data))+ ';\\n'\n",
    "\n",
    "    c_str+='unsigned char'+ var_name + '[] = {'\n",
    "    hex_array = []\n",
    "    for i, val in enumerate(hex_data):\n",
    "\n",
    "        hex_str = format(val, '#04x')\n",
    "\n",
    "        if (i+1)<len(hex_data):\n",
    "            hex_str+=','\n",
    "        if (i+1)%12 ==0:\n",
    "            hex_str+='\\n'\n",
    "        hex_array.append(hex_str)\n",
    "\n",
    "        c_str+= '\\n'+format (''.join(hex_array))+ '\\n};\\n\\n'\n",
    "\n",
    "        c_str+= '#endif //' + var_name.upper()+'_H'\n",
    "\n",
    "        return c\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfb2522d-d547-4835-9dc3-10b4968e8a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"gender_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Get input and output details\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Function to predict gender using TFLite\n",
    "def predict_gender_tflite(face_img):\n",
    "    # face_img should be RGB, normalized, shape = (48,48,3) or (64,64,3) depending on your model\n",
    "    input_shape = input_details[0]['shape']\n",
    "    face_img = cv2.resize(face_img, (input_shape[1], input_shape[2]))\n",
    "    face_img = face_img / 255.0\n",
    "    face_img = np.expand_dims(face_img, axis=0).astype(np.float32)\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], face_img)\n",
    "    interpreter.invoke()\n",
    "    pred = interpreter.get_tensor(output_details[0]['index'])[0][0]\n",
    "\n",
    "    gender_label = \"Female\" if pred >= 0.5 else \"Male\"\n",
    "    confidence = pred if pred >= 0.5 else 1 - pred\n",
    "    return gender_label, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9c4a357-f777-42c4-a620-0bd04d30ba56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model size: 0.16 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "tflite_model_path = \"gender_model.tflite\"\n",
    "\n",
    "# Get size in MB\n",
    "model_size = os.path.getsize(tflite_model_path) / (1024 * 1024)\n",
    "print(f\"TFLite model size: {model_size:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db641e1-ea24-4420-9615-4578148a5086",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = \"gender_model.tflite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25303eed-7e62-46a4-849f-0e17fba5dcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder exists or created: C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = r\"C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "print(\"Folder exists or created:\", folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d499d1-352d-4a49-84c3-d1fbe9c19a2a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\\gender_model.tflite",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m tflite_model_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDishang\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDocuments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mArduino\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtiny_ml\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mgender_model.tflite\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(tflite_model_path):\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtflite_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File not found: C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\\gender_model.tflite"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Use the exact file name here\n",
    "tflite_model_path = r\"C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\\gender_model.tflite\"\n",
    "\n",
    "if not os.path.exists(tflite_model_path):\n",
    "    raise FileNotFoundError(f\"File not found: {tflite_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "520182fd-f9b8-4a0a-8338-62b9ec1ea6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Dishang\\\\Documents\\\\Arduino\\\\tiny_ml\\\\gender_model.tflite'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtflite_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     tflite_model = f.read()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ TFLite model loaded, size:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tflite_model)/\u001b[32m1024\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\tf_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Dishang\\\\Documents\\\\Arduino\\\\tiny_ml\\\\gender_model.tflite'"
     ]
    }
   ],
   "source": [
    "with open(tflite_model_path, \"rb\") as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "print(\"✅ TFLite model loaded, size:\", len(tflite_model)/1024, \"KB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f456cf17-c5af-4229-af6f-c450210592d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Dishang\\\\Documents\\\\Arduino\\\\tiny_ml\\\\tiny_ml\\\\gender_model.tflite'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m tflite_model_path = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mC:\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mUsers\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDishang\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mDocuments\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mArduino\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtiny_ml\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mtiny_ml\u001b[39m\u001b[33m\\\u001b[39m\u001b[33mgender_model.tflite\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Load the TFLite model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtflite_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      8\u001b[39m     tflite_model = f.read()\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m✅ TFLite model loaded, size:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(tflite_model)/\u001b[32m1024\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mKB\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\tf_env\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Dishang\\\\Documents\\\\Arduino\\\\tiny_ml\\\\tiny_ml\\\\gender_model.tflite'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to your TFLite model\n",
    "tflite_model_path = r\"C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\\tiny_ml\\gender_model.tflite\"\n",
    "\n",
    "# Load the TFLite model\n",
    "with open(tflite_model_path, \"rb\") as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "print(\"✅ TFLite model loaded, size:\", len(tflite_model)/1024, \"KB\")\n",
    "\n",
    "# Path to save the generated C header file\n",
    "header_path = r\"C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\\gender_model.h\"\n",
    "folder_path = os.path.dirname(header_path)\n",
    "\n",
    "# Ensure folder exists\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "    print(\"✅ Folder created:\", folder_path)\n",
    "else:\n",
    "    print(\"✅ Folder exists:\", folder_path)\n",
    "\n",
    "# Function to convert TFLite bytes to C array\n",
    "def tflite_to_c_array(tflite_bytes, array_name=\"model_data\"):\n",
    "    c_code = f\"const unsigned char {array_name}[] = {{\\n\"\n",
    "    for i, b in enumerate(tflite_bytes):\n",
    "        if i % 12 == 0:\n",
    "            c_code += \"  \"\n",
    "        c_code += f\"0x{b:02x}, \"\n",
    "        if (i + 1) % 12 == 0:\n",
    "            c_code += \"\\n\"\n",
    "    c_code += \"\\n};\\n\"\n",
    "    c_code += f\"const unsigned int {array_name}_len = {len(tflite_bytes)};\\n\"\n",
    "    return c_code\n",
    "\n",
    "# Convert and save to header file\n",
    "try:\n",
    "    with open(header_path, \"w\") as f:\n",
    "        f.write(tflite_to_c_array(tflite_model, \"gender_model_data\"))\n",
    "    print(\"✅ Header file created:\", header_path)\n",
    "    print(\"Header file size (KB):\", os.path.getsize(header_path)/1024)\n",
    "except Exception as e:\n",
    "    print(\"❌ Error writing header file:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f4b801b2-3d9d-4482-b015-b8d79fad1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = r\"C:\\tiny_ml\\gender_model.tflite\"\n",
    "header_path = r\"C:\\tiny_ml\\gender_model.h\"\n",
    "os.makedirs(os.path.dirname(header_path), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f3d54621-3b49-43ac-9ad6-92e45e36ea93",
   "metadata": {},
   "outputs": [],
   "source": [
    "tflite_model_path = r\"C:\\Users\\Dishang\\Documents\\Arduino\\tiny_ml\\gender_model.tflite\"\n",
    "header_path = r\"C:\\tiny_ml\\gender_model.h\"  # This is where the header will be created\n",
    "os.makedirs(os.path.dirname(header_path), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "742c0fb1-1372-4c03-99e7-a6dca71cf30d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TFLite model loaded, size: 166.390625 KB\n",
      "✅ Header file created: C:\\tiny_ml\\gender_model.h\n",
      "Header file size (KB): 1053.9072265625\n"
     ]
    }
   ],
   "source": [
    "# Load the TFLite model\n",
    "with open(tflite_model_path, \"rb\") as f:\n",
    "    tflite_model = f.read()\n",
    "\n",
    "print(\"✅ TFLite model loaded, size:\", len(tflite_model)/1024, \"KB\")\n",
    "\n",
    "# Convert to C array\n",
    "c_array_code = tflite_to_c_array(tflite_model, \"gender_model_data\")\n",
    "\n",
    "# Save header\n",
    "with open(header_path, \"w\") as f:\n",
    "    f.write(c_array_code)\n",
    "\n",
    "print(\"✅ Header file created:\", header_path)\n",
    "print(\"Header file size (KB):\", os.path.getsize(header_path)/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddccef8-29c6-49da-9f60-8b1becc5a044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9321116f-d514-448d-99db-f61ea96de2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate total parameters in TFLite model: 1070117\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=tflite_model_path)\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# Count total parameters\n",
    "total_params = 0\n",
    "for detail in interpreter.get_tensor_details():\n",
    "    shape = detail['shape']\n",
    "    total_params += np.prod(shape)\n",
    "\n",
    "print(f\"Approximate total parameters in TFLite model: {total_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01c8f369-4ee6-4494-9711-362fa3791d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # webcam\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        face_img = frame[y:y+h, x:x+w]\n",
    "        face_rgb = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)\n",
    "        gender, conf = predict_gender_tflite(face_rgb)\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)\n",
    "        cv2.putText(frame, f\"{gender} ({conf*100:.1f}%)\", (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,255,0), 2)\n",
    "\n",
    "    cv2.imshow(\"TFLite Gender Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101a6e3-a666-40a4-abee-1d3054e3256d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
